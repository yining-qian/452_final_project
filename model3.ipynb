{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bd008e",
   "metadata": {},
   "source": [
    "### Data extraction and feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a79d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7be1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_note_features_with_instrument(midi_file):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        notes = defaultdict(list)\n",
    "        \n",
    "        if not pm.instruments:\n",
    "            raise ValueError(\"No instruments found in MIDI file.\")\n",
    "\n",
    "        for instrument in pm.instruments:\n",
    "            program_num = instrument.program if not instrument.is_drum else 127  # Assign drum as 128ï¼Œnow change to 127\n",
    "            #program_num = instrument.program  # <- Extract instrument ID\n",
    "            for note in instrument.notes:\n",
    "                notes[\"pitch\"].append(note.pitch)\n",
    "                notes[\"velocity\"].append(note.velocity)\n",
    "                notes[\"note_name\"].append(pretty_midi.note_number_to_name(note.pitch))  # e.g., 'C#4'\n",
    "                notes[\"octave\"].append(note.pitch // 12 - 1)  # Convert MIDI pitch to octave number\n",
    "                notes[\"start\"].append(note.start)\n",
    "                notes[\"end\"].append(note.end)\n",
    "                notes[\"duration\"].append(note.end - note.start)\n",
    "                notes[\"instrument\"].append(program_num)  # <- Add this line\n",
    "\n",
    "        return pd.DataFrame(notes)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {midi_file} due to error: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199d963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_note_features(midi_file):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        instrument = pm.instruments[0]  # Assuming single instrument for now\n",
    "\n",
    "        notes = defaultdict(list)\n",
    "        for note in instrument.notes:\n",
    "            notes[\"pitch\"].append(note.pitch)\n",
    "            notes[\"velocity\"].append(note.velocity)  # Extract actual velocity (1-127)\n",
    "            notes[\"note_name\"].append(pretty_midi.note_number_to_name(note.pitch))  # e.g., 'C#4'\n",
    "            notes[\"octave\"].append(note.pitch // 12 - 1)  # Convert MIDI pitch to octave number\n",
    "            notes[\"start\"].append(note.start)\n",
    "            notes[\"end\"].append(note.end)\n",
    "            notes[\"duration\"].append(note.end - note.start)            \n",
    "\n",
    "        return pd.DataFrame(notes)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {midi_file}: {e}\")\n",
    "        return pd.DataFrame()  # Return empty if failed\n",
    "    \n",
    "    \n",
    "def extract_all_midi_files(folder, max_files=500):\n",
    "    paths = list(Path(folder).rglob(\"*.[mM][iI][dD]*\"))[:max_files]\n",
    "    print(f\"Found {len(paths)} MIDI files\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for path in paths:\n",
    "        df = extract_advanced_note_features(str(path))\n",
    "        if not df.empty:\n",
    "            df[\"filename\"] = path.name\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135d991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 MIDI files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>F5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>1.087240</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.108073</td>\n",
       "      <td>1.173177</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>G#4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>1.268229</td>\n",
       "      <td>0.061198</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.315104</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>C#3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998698</td>\n",
       "      <td>1.401042</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     77        56        F5       5  0.994792  1.087240  0.092448   \n",
       "1     73        58       C#5       5  1.108073  1.173177  0.065104   \n",
       "2     68        58       G#4       4  1.207031  1.268229  0.061198   \n",
       "3     73        62       C#5       5  1.315104  1.375000  0.059896   \n",
       "4     49        32       C#3       3  0.998698  1.401042  0.402344   \n",
       "\n",
       "                                            filename  \n",
       "0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "1  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "2  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "3  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "4  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data and extract pitch, velocity, note_name, octave, start_time, end, and duration \n",
    "data_dir = \"/Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/Maestro/maestro-v3.0.0\"\n",
    "all_notes_df = extract_all_midi_files(data_dir)\n",
    "all_notes_df.head()\n",
    "\n",
    "#all_notes_df.to_csv(\"/Users/yang/Documents/processed_notes_lakh.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca7047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_notes_df[\"filename\"].nunique() # check number of unique files in our code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dafbe0",
   "metadata": {},
   "source": [
    "## Version 2: try to extract instrument information correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbac7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_all_midi_files_3(folder, max_files=500): # this works \n",
    "    paths = list(Path(folder).rglob(\"*.[mM][iI][dD]*\"))[:max_files]\n",
    "    print(f\"Found {len(paths)} MIDI files\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for path in paths:\n",
    "        df = extract_note_features_with_instrument(str(path))\n",
    "        if not df.empty:\n",
    "            df[\"filename\"] = path.name\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4e2e3",
   "metadata": {},
   "source": [
    "**Successfully extract the instrument information now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d93570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 MIDI files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>instrument</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>F5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>1.087240</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.108073</td>\n",
       "      <td>1.173177</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>G#4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>1.268229</td>\n",
       "      <td>0.061198</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.315104</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>C#3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998698</td>\n",
       "      <td>1.401042</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     77        56        F5       5  0.994792  1.087240  0.092448   \n",
       "1     73        58       C#5       5  1.108073  1.173177  0.065104   \n",
       "2     68        58       G#4       4  1.207031  1.268229  0.061198   \n",
       "3     73        62       C#5       5  1.315104  1.375000  0.059896   \n",
       "4     49        32       C#3       3  0.998698  1.401042  0.402344   \n",
       "\n",
       "   instrument                                           filename  \n",
       "0           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "1           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "2           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "3           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  \n",
       "4           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second version that also extract information related to instrument used \n",
    "\n",
    "data_dir = \"/Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/Maestro/maestro-v3.0.0\"\n",
    "extracted_mae_df = extract_all_midi_files_3(data_dir)\n",
    "extracted_mae_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_mae_df.to_csv(\"/Users/yang/Documents/processed_notes_lakh_instru.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c542c26",
   "metadata": {},
   "source": [
    "### Data preparation & Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7c7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline for symbolic MusicGen-style training using extracted note features (with chord conditioning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "D_MODEL = 256\n",
    "NUM_PITCHES = 128\n",
    "NUM_VELOCITIES = 32\n",
    "NUM_DURATIONS = 32\n",
    "NUM_CHORDS = 12  # 12 pitch classes (C, C#, D, ..., B)\n",
    "NUM_INSTRUMENTS = 128\n",
    "SEQ_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bf1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chord Estimation from Notes (chromagram-inspired)\n",
    "def estimate_chords(df):\n",
    "    df = df.copy()\n",
    "    df[\"chord\"] = -1\n",
    "    filenames = df[\"filename\"].unique()\n",
    "    for fname in filenames:\n",
    "        song = df[df[\"filename\"] == fname].copy()\n",
    "        song = song.sort_values(\"start\")\n",
    "        chords = []\n",
    "        for i in range(0, len(song), SEQ_LEN):\n",
    "            segment = song.iloc[i:i+SEQ_LEN]\n",
    "            pitch_classes = [p % 12 for p in segment[\"pitch\"]]\n",
    "            if len(pitch_classes) == 0:\n",
    "                chord_id = 0\n",
    "            else:\n",
    "                chord_id = Counter(pitch_classes).most_common(1)[0][0]\n",
    "            chords += [chord_id] * len(segment)\n",
    "        df.loc[df[\"filename\"] == fname, \"chord\"] = chords\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6dce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation \n",
    "def discretize_velocity(velocity):\n",
    "    return min(int(velocity // 4), NUM_VELOCITIES - 1)\n",
    "\n",
    "def discretize_duration(duration):\n",
    "    \n",
    "    idx = np.floor(duration / 0.1).astype(int)\n",
    "    # return min(int(duration / 0.1), NUM_DURATIONS - 1) # original version, change to avoid overflow\n",
    "    return np.clip(idx, 0, NUM_DURATIONS - 1)\n",
    "\n",
    "def build_sequence_tensor(df, max_seq_len=SEQ_LEN):\n",
    "    sequences = []\n",
    "    grouped = df.groupby(\"filename\")\n",
    "    \n",
    "    # For debug: record all values\n",
    "    all_durations = []\n",
    "    all_velocities = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values(\"start\")\n",
    "        for i in range(0, len(group) - max_seq_len, max_seq_len):\n",
    "            chunk = group.iloc[i:i+max_seq_len]\n",
    "            pitch = torch.tensor(chunk[\"pitch\"].values, dtype=torch.long)\n",
    "            velocity = torch.tensor(chunk[\"velocity\"].apply(discretize_velocity).values, dtype=torch.long)\n",
    "            duration = torch.tensor(chunk[\"duration\"].apply(discretize_duration).values, dtype=torch.long)\n",
    "            chord = torch.tensor(chunk[\"chord\"].values, dtype=torch.long)\n",
    "            instrument = torch.tensor(chunk[\"instrument\"].values, dtype=torch.long)\n",
    "            sequences.append((pitch, velocity, duration, chord, instrument)) # cannot successfully obtain instru\n",
    "            \n",
    "            # save for debug\n",
    "            all_durations.extend(duration.tolist())\n",
    "            all_velocities.extend(velocity.tolist())\n",
    "            \n",
    "            \n",
    "    # After discretization inside build_sequence_tensor\n",
    "    print(\"Duration max:\", max(all_durations))\n",
    "    print(\"Duration min:\", min(all_durations))\n",
    "    print(\"Velocity max:\", max(all_velocities))\n",
    "    print(\"Velocity min:\", min(all_velocities))\n",
    "\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d8e9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset Wrapper\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b1e517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration max: 31\n",
      "Duration min: 0\n",
      "Velocity max: 31\n",
      "Velocity min: 0\n"
     ]
    }
   ],
   "source": [
    "#df = estimate_chords(all_notes_df)\n",
    "df = estimate_chords(extracted_mae_df)\n",
    "sequences = build_sequence_tensor(df)\n",
    "dataset = SequenceDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc4d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>instrument</th>\n",
       "      <th>filename</th>\n",
       "      <th>chord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>56</td>\n",
       "      <td>F5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>1.087240</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.108073</td>\n",
       "      <td>1.173177</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>58</td>\n",
       "      <td>G#4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>1.268229</td>\n",
       "      <td>0.061198</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>62</td>\n",
       "      <td>C#5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.315104</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>C#3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998698</td>\n",
       "      <td>1.401042</td>\n",
       "      <td>0.402344</td>\n",
       "      <td>0</td>\n",
       "      <td>ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     77        56        F5       5  0.994792  1.087240  0.092448   \n",
       "1     73        58       C#5       5  1.108073  1.173177  0.065104   \n",
       "2     68        58       G#4       4  1.207031  1.268229  0.061198   \n",
       "3     73        62       C#5       5  1.315104  1.375000  0.059896   \n",
       "4     49        32       C#3       3  0.998698  1.401042  0.402344   \n",
       "\n",
       "   instrument                                           filename  chord  \n",
       "0           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...      1  \n",
       "1           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...      1  \n",
       "2           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...      1  \n",
       "3           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...      1  \n",
       "4           0  ORIG-MIDI_01_7_7_13_Group__MID--AUDIO_12_R1_20...      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c4e29",
   "metadata": {},
   "source": [
    "## Import and use REMI tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db99ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import miditok\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = miditok.REMI()  # you can also try TSD, CPWord, Octuple, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create tokenizer\n",
    "from miditok import REMI, TokenizerConfig\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True) # change the use_program = False for 1 instrument \n",
    "tokenizer = REMI(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e67e2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditoolkit import MidiFile\n",
    "from pathlib import Path\n",
    "from symusic import Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acb9d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1276 MIDI files.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create REMI tokenizer\n",
    "config = TokenizerConfig(\n",
    "    num_velocities=16,\n",
    "    use_chords=True,\n",
    "    use_programs=True\n",
    ")\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# 2. Extract all MIDI files\n",
    "data_dir = Path(\"/Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/Maestro/maestro-v3.0.0\")\n",
    "paths = list(data_dir.rglob(\"*.[mM][iI][dD]*\"))\n",
    "print(f\"Found {len(paths)} MIDI files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce13084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize midis\n",
    "token_seqs = []\n",
    "for path in paths:\n",
    "    midi = Score(str(path))   # <=== MUST use Score now\n",
    "    tokens = tokenizer(midi)\n",
    "    token_seqs.append(tokens.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc2352",
   "metadata": {},
   "source": [
    "# Revising model (complex version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fca811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "'''\n",
    "d_model = dimension of my model embeddings = size of the vector for each token at every time step\n",
    "\n",
    "'''\n",
    "\n",
    "# updated model \n",
    "# ==== Relative Multihead Attention ====\n",
    "class RelativeMultiHeadAttn(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head  # divide so that each head work on subvector \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False) # project input vectors of size d_model in to qkv\n",
    "        self.o = nn.Linear(d_model, d_model, bias=False) # project concatenated multi-head back to original embedding \n",
    "        self.r_r_bias = nn.Parameter(torch.randn(n_head, self.d_head)) # initialize with random small value \n",
    "        self.r_w_bias = nn.Parameter(torch.randn(n_head, self.d_head))\n",
    "        self.rel_embed = nn.Embedding(512, self.d_head)  # relative position embedding\n",
    "\n",
    "    def forward(self, h, mask=None):\n",
    "        B, T, D = h.shape\n",
    "        qkv = self.qkv(h).view(B, T, 3, self.n_head, self.d_head)\n",
    "        q, k, v = qkv.unbind(dim=2)  # shapes: (B, T, n_head, d_head)\n",
    "\n",
    "        AC = torch.einsum('bthd,bThd->bhtT', (q + self.r_w_bias, k))\n",
    "\n",
    "        positions = torch.arange(T, device=h.device)\n",
    "        rel = positions[None, :] - positions[:, None]\n",
    "        rel = rel.clamp(min=0, max=511)\n",
    "        r = self.rel_embed(rel)\n",
    "        BD = torch.einsum('bthd,Ttd->bhtT', (q + self.r_r_bias, r))\n",
    "\n",
    "        scores = (AC + BD) / math.sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask[:, None, None, :], -1e9)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.einsum('bhtT,bThd->bthd', attn, v)\n",
    "        out = out.contiguous().view(B, T, D)\n",
    "        return self.o(out)\n",
    "\n",
    "# ==== FiLM Layer ====\n",
    "class FiLMLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.to_gamma = nn.Linear(d_model, d_model)\n",
    "        self.to_beta = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, chord_emb):\n",
    "        if chord_emb is None:\n",
    "            return x  # Skip if no chord conditioning (during generation)\n",
    "        gamma = torch.tanh(self.to_gamma(chord_emb))\n",
    "        beta = self.to_beta(chord_emb)\n",
    "        return gamma * x + beta\n",
    "\n",
    "# ==== Decoder Layer ====\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = RelativeMultiHeadAttn(d_model, n_head)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.film = FiLMLayer(d_model)\n",
    "\n",
    "    def forward(self, x, chord_emb, mask=None):\n",
    "        h = self.attn(self.ln1(x), mask)\n",
    "        x = x + h\n",
    "        x = self.film(x, chord_emb)\n",
    "        h2 = self.ff(self.ln2(x))\n",
    "        return x + h2\n",
    "\n",
    "# ==== Chord Encoder ====\n",
    "class ChordEncoder(nn.Module):\n",
    "    def __init__(self, d_model=256, n_layer=2, n_head=4):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(13, d_model)  # 12 roots + NO_CHORD\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, n_head, d_model * 4, batch_first=True)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, chord_seq):\n",
    "        x = self.emb(chord_seq)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        pooled = self.pool(x.transpose(1, 2)).squeeze(-1)\n",
    "        return pooled\n",
    "\n",
    "# ==== Full Music Transformer ====\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_layer=8, n_head=8, d_ff=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1024, d_model))\n",
    "        self.token_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_head, d_ff)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.chord_encoder = ChordEncoder(d_model=d_model // 2)\n",
    "        self.proj_chord = nn.Linear(d_model // 2, d_model)\n",
    "\n",
    "    def forward(self, tokens, chord_seq=None, mask=None):\n",
    "        B, T = tokens.shape\n",
    "\n",
    "        x = self.token_emb(tokens) + self.pos_emb[:, :T, :]\n",
    "        x = self.token_ln(x)\n",
    "\n",
    "        chord_emb = None\n",
    "        if chord_seq is not None:\n",
    "            chord_emb = self.proj_chord(self.chord_encoder(chord_seq)).unsqueeze(1)  # (B, 1, d_model)\n",
    "\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x, chord_emb, mask)\n",
    "\n",
    "        h = self.ln_final(x)\n",
    "        return self.output_head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a5df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df, seq_len=128):\n",
    "        self.seq_len = seq_len\n",
    "        self.samples = []\n",
    "\n",
    "        # Group by filename (each file is a sequence)\n",
    "        for fname, group in df.groupby('filename'):\n",
    "            pitch = torch.tensor(group['pitch'].values, dtype=torch.long)\n",
    "            velocity = torch.tensor(group['velocity'].values, dtype=torch.long)\n",
    "            duration = torch.tensor(group['duration'].values, dtype=torch.long)  # make sure it is already int mapped\n",
    "            instrument = torch.tensor(group['instrument'].values, dtype=torch.long)\n",
    "            chord = torch.tensor(group['chord'].values, dtype=torch.long)\n",
    "\n",
    "            # Create sliding windows\n",
    "            total_len = pitch.shape[0]\n",
    "            if total_len >= seq_len + 1:\n",
    "                for i in range(0, total_len - seq_len):\n",
    "                    self.samples.append((\n",
    "                        pitch[i:i+seq_len],\n",
    "                        velocity[i:i+seq_len],\n",
    "                        duration[i:i+seq_len],\n",
    "                        instrument[i:i+seq_len],\n",
    "                        chord[i:i+seq_len],\n",
    "                        pitch[i+1:i+1+seq_len],\n",
    "                        velocity[i+1:i+1+seq_len],\n",
    "                        duration[i+1:i+1+seq_len],\n",
    "                        instrument[i+1:i+1+seq_len],\n",
    "                    ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a9e665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(df, seq_len=128)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00c314f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=5, lr=1e-4, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Now batch is simple (B, T)\n",
    "            tokens = batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Predict next token for each position\n",
    "            logits = model(tokens[:, :-1])  # input tokens except last\n",
    "            target = tokens[:, 1:]          # target tokens shifted by one\n",
    "\n",
    "            B, T, vocab_size = logits.shape\n",
    "\n",
    "            loss = criterion(logits.reshape(B*T, vocab_size), target.reshape(B*T))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613e5cd",
   "metadata": {},
   "source": [
    "## Try to train on REMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fb4a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MusicTokenDataset(Dataset):\n",
    "    def __init__(self, token_seqs, max_seq_len=1024):\n",
    "        self.samples = []\n",
    "        for tokens in token_seqs:\n",
    "            # break long tokens into smaller chunks\n",
    "            for i in range(0, len(tokens) - 1, max_seq_len):\n",
    "                chunk = tokens[i:i+max_seq_len+1]\n",
    "                if len(chunk) > 1:  # at least (input, output)\n",
    "                    self.samples.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.tensor(self.samples[idx], dtype=torch.long)\n",
    "        return tokens[:-1], tokens[1:]  # (input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "174fc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MusicTokenDataset(token_seqs, max_seq_len=1024)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "122be68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MusicTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,   # single unified vocab size!\n",
    "    d_model=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    d_ff=2048\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2699893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_music(model, tokenizer, max_steps=512, device='cpu', start_tokens=None):\n",
    "    model.eval()\n",
    "    generated = []\n",
    "\n",
    "    if start_tokens is None:\n",
    "        # random start: pick a valid token\n",
    "        start_token = torch.randint(0, tokenizer.vocab_size, (1,), device=device).item()\n",
    "        generated = [start_token]\n",
    "    else:\n",
    "        generated = start_tokens\n",
    "\n",
    "    generated = torch.tensor(generated, device=device).unsqueeze(0)  # shape (1, T)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        logits = model(generated)  # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :]  # Take the last time step's logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated.squeeze(0).tolist()  # return as list of token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6108282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = generate_music(model, tokenizer, max_steps=512, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca2b9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import TokSequence\n",
    "\n",
    "# Create a TokSequence\n",
    "generated_seq = TokSequence(ids=tokens)\n",
    "\n",
    "# Decode back to MIDI Score\n",
    "generated_score = tokenizer(generated_seq)\n",
    "\n",
    "# Save to MIDI file\n",
    "generated_score.dump_midi(\"generated_music_10.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88c4aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_music_conditioned(model, tokenizer, chord_seq, max_steps=512, device='cpu', start_tokens=None):\n",
    "    model.eval()\n",
    "\n",
    "    generated = []\n",
    "    if start_tokens is None:\n",
    "        start_token = torch.randint(0, tokenizer.vocab_size, (1,), device=device).item()\n",
    "        generated = [start_token]\n",
    "    else:\n",
    "        generated = start_tokens\n",
    "\n",
    "    generated = torch.tensor(generated, device=device).unsqueeze(0)  # (1, T)\n",
    "\n",
    "    chord_seq = torch.tensor(chord_seq, device=device).unsqueeze(0)  # (1, T_chords)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Slice the chord token to feed\n",
    "        if step < chord_seq.shape[1]:\n",
    "            current_chord = chord_seq[:, step].unsqueeze(1)  # (B=1, T=1)\n",
    "        else:\n",
    "            current_chord = chord_seq[:, -1].unsqueeze(1)  # Repeat last chord if out of range\n",
    "\n",
    "        logits = model(generated, chord_seq=current_chord)  # forward with chord conditioning\n",
    "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa25ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example chord sequence: [C major, G major, A minor, F major]\n",
    "# Assume chord classes are like {0: C, 1: C#, 2: D, ..., 11: B, 12: no-chord}\n",
    "\n",
    "chord_progression = [0, 7, 9, 5]  # C, G, A, F\n",
    "\n",
    "# Generate music conditioned on this chord progression\n",
    "tokens = generate_music_conditioned(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    chord_seq=chord_progression,   # your given chord list\n",
    "    max_steps=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Then decode the same way:\n",
    "from miditok import TokSequence\n",
    "generated_seq = TokSequence(ids=tokens)\n",
    "generated_score = tokenizer(generated_seq)\n",
    "generated_score.dump_midi(\"generated_conditioned_1.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
