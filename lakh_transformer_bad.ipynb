{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bd008e",
   "metadata": {},
   "source": [
    "### Data extraction and feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a79d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7be1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_note_features_with_instrument(midi_file):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        notes = defaultdict(list)\n",
    "        \n",
    "        if not pm.instruments:\n",
    "            raise ValueError(\"No instruments found in MIDI file.\")\n",
    "\n",
    "        for instrument in pm.instruments:\n",
    "            program_num = instrument.program if not instrument.is_drum else 127  # Assign drum as 128ï¼Œnow change to 127\n",
    "            #program_num = instrument.program  # <- Extract instrument ID\n",
    "            for note in instrument.notes:\n",
    "                notes[\"pitch\"].append(note.pitch)\n",
    "                notes[\"velocity\"].append(note.velocity)\n",
    "                notes[\"note_name\"].append(pretty_midi.note_number_to_name(note.pitch))  # e.g., 'C#4'\n",
    "                notes[\"octave\"].append(note.pitch // 12 - 1)  # Convert MIDI pitch to octave number\n",
    "                notes[\"start\"].append(note.start)\n",
    "                notes[\"end\"].append(note.end)\n",
    "                notes[\"duration\"].append(note.end - note.start)\n",
    "                notes[\"instrument\"].append(program_num)  # <- Add this line\n",
    "\n",
    "        return pd.DataFrame(notes)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {midi_file} due to error: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199d963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_note_features(midi_file):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "        instrument = pm.instruments[0]  # Assuming single instrument for now\n",
    "\n",
    "        notes = defaultdict(list)\n",
    "        for note in instrument.notes:\n",
    "            notes[\"pitch\"].append(note.pitch)\n",
    "            notes[\"velocity\"].append(note.velocity)  # Extract actual velocity (1-127)\n",
    "            notes[\"note_name\"].append(pretty_midi.note_number_to_name(note.pitch))  # e.g., 'C#4'\n",
    "            notes[\"octave\"].append(note.pitch // 12 - 1)  # Convert MIDI pitch to octave number\n",
    "            notes[\"start\"].append(note.start)\n",
    "            notes[\"end\"].append(note.end)\n",
    "            notes[\"duration\"].append(note.end - note.start)            \n",
    "\n",
    "        return pd.DataFrame(notes)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {midi_file}: {e}\")\n",
    "        return pd.DataFrame()  # Return empty if failed\n",
    "    \n",
    "    \n",
    "def extract_all_midi_files(folder, max_files=100):\n",
    "    paths = list(Path(folder).rglob(\"*.mid\"))[:max_files]\n",
    "    print(f\"Found {len(paths)} MIDI files\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for path in paths:\n",
    "        df = extract_advanced_note_features(str(path))\n",
    "        if not df.empty:\n",
    "            df[\"filename\"] = path.name\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "135d991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 MIDI files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>90</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.315776</td>\n",
       "      <td>6.776301</td>\n",
       "      <td>0.460525</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.103192</td>\n",
       "      <td>7.487648</td>\n",
       "      <td>0.384456</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>91</td>\n",
       "      <td>G#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.493816</td>\n",
       "      <td>7.845378</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>83</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.880329</td>\n",
       "      <td>8.402531</td>\n",
       "      <td>0.522203</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.673912</td>\n",
       "      <td>9.083040</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     46        90       A#2       2  6.315776  6.776301  0.460525   \n",
       "1     41        92        F2       2  7.103192  7.487648  0.384456   \n",
       "2     44        91       G#2       2  7.493816  7.845378  0.351562   \n",
       "3     46        83       A#2       2  7.880329  8.402531  0.522203   \n",
       "4     41        74        F2       2  8.673912  9.083040  0.409127   \n",
       "\n",
       "                               filename  \n",
       "0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "1  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "2  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "3  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "4  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data and extract pitch, velocity, note_name, octave, start_time, end, and duration \n",
    "data_dir = \"/Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/lmd_matched\"\n",
    "all_notes_df = extract_all_midi_files(data_dir)\n",
    "all_notes_df.head()\n",
    "\n",
    "#all_notes_df.to_csv(\"/Users/yang/Documents/processed_notes_lakh.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca7047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_notes_df[\"filename\"].nunique() # check number of unique files in our code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dafbe0",
   "metadata": {},
   "source": [
    "## Version 2: try to extract instrument information correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbac7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_all_midi_files_2(root_dir, max_files=None):\n",
    "    from tqdm import tqdm\n",
    "    import pandas as pd\n",
    "\n",
    "    all_notes = []\n",
    "    count = 0\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if not file.lower().endswith(\".mid\"):\n",
    "                continue\n",
    "\n",
    "            full_path = os.path.join(dirpath, file)\n",
    "\n",
    "            df = extract_note_features_with_instrument(full_path)\n",
    "            if not df.empty:\n",
    "                df[\"filename\"] = file\n",
    "                all_notes.append(df)\n",
    "                count += 1\n",
    "\n",
    "            if max_files and count >= max_files:\n",
    "                break\n",
    "\n",
    "    return pd.concat(all_notes, ignore_index=True) if all_notes else pd.DataFrame()\n",
    "\n",
    "def extract_all_midi_files_3(folder, max_files=500): # this works \n",
    "    paths = list(Path(folder).rglob(\"*.mid\"))[:max_files]\n",
    "    print(f\"Found {len(paths)} MIDI files\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for path in paths:\n",
    "        df = extract_note_features_with_instrument(str(path))\n",
    "        if not df.empty:\n",
    "            df[\"filename\"] = path.name\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4e2e3",
   "metadata": {},
   "source": [
    "**Successfully extract the instrument information now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d93570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 MIDI files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse /Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/lmd_matched/R/U/I/TRRUIKN128E078218F/c03a94962031205a7656296e967b92f7.mid due to error: data byte must be in range 0..127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>instrument</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>90</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.315776</td>\n",
       "      <td>6.776301</td>\n",
       "      <td>0.460525</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.103192</td>\n",
       "      <td>7.487648</td>\n",
       "      <td>0.384456</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>91</td>\n",
       "      <td>G#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.493816</td>\n",
       "      <td>7.845378</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>83</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.880329</td>\n",
       "      <td>8.402531</td>\n",
       "      <td>0.522203</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.673912</td>\n",
       "      <td>9.083040</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     46        90       A#2       2  6.315776  6.776301  0.460525   \n",
       "1     41        92        F2       2  7.103192  7.487648  0.384456   \n",
       "2     44        91       G#2       2  7.493816  7.845378  0.351562   \n",
       "3     46        83       A#2       2  7.880329  8.402531  0.522203   \n",
       "4     41        74        F2       2  8.673912  9.083040  0.409127   \n",
       "\n",
       "   instrument                              filename  \n",
       "0           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "1           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "2           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "3           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  \n",
       "4           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second version that also extract information related to instrument used \n",
    "\n",
    "data_dir = \"/Users/yang/Desktop/Yale Spring 2025/CPSC 552 Deep learning theory and applications /DeepL project - music generation /Data set/lmd_matched\"\n",
    "extracted_lakh_df = extract_all_midi_files_3(data_dir)\n",
    "extracted_lakh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_lakh_df.to_csv(\"/Users/yang/Documents/processed_notes_lakh_instru.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c542c26",
   "metadata": {},
   "source": [
    "### Data preparation & Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be7c7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline for symbolic MusicGen-style training using extracted note features (with chord conditioning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "D_MODEL = 256\n",
    "NUM_PITCHES = 128\n",
    "NUM_VELOCITIES = 32\n",
    "NUM_DURATIONS = 32\n",
    "NUM_CHORDS = 12  # 12 pitch classes (C, C#, D, ..., B)\n",
    "NUM_INSTRUMENTS = 128\n",
    "SEQ_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34bf1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chord Estimation from Notes (chromagram-inspired)\n",
    "def estimate_chords(df):\n",
    "    df = df.copy()\n",
    "    df[\"chord\"] = -1\n",
    "    filenames = df[\"filename\"].unique()\n",
    "    for fname in filenames:\n",
    "        song = df[df[\"filename\"] == fname].copy()\n",
    "        song = song.sort_values(\"start\")\n",
    "        chords = []\n",
    "        for i in range(0, len(song), SEQ_LEN):\n",
    "            segment = song.iloc[i:i+SEQ_LEN]\n",
    "            pitch_classes = [p % 12 for p in segment[\"pitch\"]]\n",
    "            if len(pitch_classes) == 0:\n",
    "                chord_id = 0\n",
    "            else:\n",
    "                chord_id = Counter(pitch_classes).most_common(1)[0][0]\n",
    "            chords += [chord_id] * len(segment)\n",
    "        df.loc[df[\"filename\"] == fname, \"chord\"] = chords\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b6dce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation \n",
    "def discretize_velocity(velocity):\n",
    "    return min(int(velocity // 4), NUM_VELOCITIES - 1)\n",
    "\n",
    "def discretize_duration(duration):\n",
    "    \n",
    "    idx = np.floor(duration / 0.1).astype(int)\n",
    "    # return min(int(duration / 0.1), NUM_DURATIONS - 1) # original version, change to avoid overflow\n",
    "    return np.clip(idx, 0, NUM_DURATIONS - 1)\n",
    "\n",
    "def build_sequence_tensor(df, max_seq_len=SEQ_LEN):\n",
    "    sequences = []\n",
    "    grouped = df.groupby(\"filename\")\n",
    "    \n",
    "    # For debug: record all values\n",
    "    all_durations = []\n",
    "    all_velocities = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values(\"start\")\n",
    "        for i in range(0, len(group) - max_seq_len, max_seq_len):\n",
    "            chunk = group.iloc[i:i+max_seq_len]\n",
    "            pitch = torch.tensor(chunk[\"pitch\"].values, dtype=torch.long)\n",
    "            velocity = torch.tensor(chunk[\"velocity\"].apply(discretize_velocity).values, dtype=torch.long)\n",
    "            duration = torch.tensor(chunk[\"duration\"].apply(discretize_duration).values, dtype=torch.long)\n",
    "            chord = torch.tensor(chunk[\"chord\"].values, dtype=torch.long)\n",
    "            instrument = torch.tensor(chunk[\"instrument\"].values, dtype=torch.long)\n",
    "            sequences.append((pitch, velocity, duration, chord, instrument)) # cannot successfully obtain instru\n",
    "            \n",
    "            # save for debug\n",
    "            all_durations.extend(duration.tolist())\n",
    "            all_velocities.extend(velocity.tolist())\n",
    "            \n",
    "            \n",
    "    # After discretization inside build_sequence_tensor\n",
    "    print(\"Duration max:\", max(all_durations))\n",
    "    print(\"Duration min:\", min(all_durations))\n",
    "    print(\"Velocity max:\", max(all_velocities))\n",
    "    print(\"Velocity min:\", min(all_velocities))\n",
    "\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8e9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset Wrapper\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27b1e517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration max: 31\n",
      "Duration min: 0\n",
      "Velocity max: 31\n",
      "Velocity min: 0\n"
     ]
    }
   ],
   "source": [
    "#df = estimate_chords(all_notes_df)\n",
    "df = estimate_chords(extracted_lakh_df)\n",
    "sequences = build_sequence_tensor(df)\n",
    "dataset = SequenceDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecc4d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>velocity</th>\n",
       "      <th>note_name</th>\n",
       "      <th>octave</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>instrument</th>\n",
       "      <th>filename</th>\n",
       "      <th>chord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>90</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.315776</td>\n",
       "      <td>6.776301</td>\n",
       "      <td>0.460525</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.103192</td>\n",
       "      <td>7.487648</td>\n",
       "      <td>0.384456</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>91</td>\n",
       "      <td>G#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.493816</td>\n",
       "      <td>7.845378</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>83</td>\n",
       "      <td>A#2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.880329</td>\n",
       "      <td>8.402531</td>\n",
       "      <td>0.522203</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>F2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.673912</td>\n",
       "      <td>9.083040</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>0</td>\n",
       "      <td>2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pitch  velocity note_name  octave     start       end  duration  \\\n",
       "0     46        90       A#2       2  6.315776  6.776301  0.460525   \n",
       "1     41        92        F2       2  7.103192  7.487648  0.384456   \n",
       "2     44        91       G#2       2  7.493816  7.845378  0.351562   \n",
       "3     46        83       A#2       2  7.880329  8.402531  0.522203   \n",
       "4     41        74        F2       2  8.673912  9.083040  0.409127   \n",
       "\n",
       "   instrument                              filename  chord  \n",
       "0           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid     10  \n",
       "1           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid     10  \n",
       "2           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid     10  \n",
       "3           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid     10  \n",
       "4           0  2740bc2a1cd9bae5dfb5ddc40f2aefb9.mid     10  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89b841",
   "metadata": {},
   "source": [
    "### Model construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93b61ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition (Chord Conditioning)\n",
    "class SymbolicMusicTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pitch_embed = nn.Embedding(NUM_PITCHES, D_MODEL)\n",
    "        self.velocity_embed = nn.Embedding(NUM_VELOCITIES, D_MODEL)\n",
    "        self.duration_embed = nn.Embedding(NUM_DURATIONS, D_MODEL)\n",
    "        self.chord_embed = nn.Embedding(NUM_CHORDS, D_MODEL)\n",
    "        self.instrument_embed = nn.Embedding(NUM_INSTRUMENTS, D_MODEL)\n",
    "        self.pos_embed = nn.Embedding(SEQ_LEN, D_MODEL)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=8, dim_feedforward=512)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "        self.pitch_out = nn.Linear(D_MODEL, NUM_PITCHES)\n",
    "        self.velocity_out = nn.Linear(D_MODEL, NUM_VELOCITIES)\n",
    "        self.duration_out = nn.Linear(D_MODEL, NUM_DURATIONS)\n",
    "        self.instrument_out = nn.Linear(D_MODEL, NUM_INSTRUMENTS)\n",
    "\n",
    "    def forward(self, pitch, velocity, duration, chord, instrument):\n",
    "        B, T = pitch.shape\n",
    "        pos = torch.arange(T, device=pitch.device).unsqueeze(0).expand(B, T)\n",
    "\n",
    "        x = self.pitch_embed(pitch) + \\\n",
    "            self.velocity_embed(velocity) + \\\n",
    "            self.duration_embed(duration) + \\\n",
    "            self.chord_embed(chord) + \\\n",
    "            self.instrument_embed(instrument) + \\\n",
    "            self.pos_embed(pos)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return self.pitch_out(x), self.velocity_out(x), self.duration_out(x), self.instrument_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "051cab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataloader, epochs=7):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for pitch, velocity, duration, chord, instrument in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pitch_logits, vel_logits, dur_logits, instr_logits = model(pitch, velocity, duration, chord, instrument)\n",
    "            \n",
    "            loss = F.cross_entropy(pitch_logits.view(-1, NUM_PITCHES), pitch.view(-1)) + \\\n",
    "                   F.cross_entropy(vel_logits.view(-1, NUM_VELOCITIES), velocity.view(-1)) + \\\n",
    "                   F.cross_entropy(dur_logits.view(-1, NUM_DURATIONS), duration.view(-1)) + \\\n",
    "                   F.cross_entropy(instr_logits.view(-1, NUM_INSTRUMENTS), instrument.view(-1))\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbef716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch range: 123 0\n",
      "Velocity range: 127 1\n",
      "Duration range: 219.42774187500004 4.844958333194427e-05\n",
      "Chord range: 11 0\n",
      "Instrument range: 127 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pitch range:\", df[\"pitch\"].max(), df[\"pitch\"].min())\n",
    "print(\"Velocity range:\", df[\"velocity\"].max(), df[\"velocity\"].min())\n",
    "print(\"Duration range:\", df[\"duration\"].max(), df[\"duration\"].min())\n",
    "print(\"Chord range:\", df[\"chord\"].max(), df[\"chord\"].min())\n",
    "print(\"Instrument range:\", df[\"instrument\"].max(), df[\"instrument\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1961db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4242\n",
      "Epoch 2, Loss: 0.0031\n",
      "Epoch 3, Loss: 0.0010\n",
      "Epoch 4, Loss: 0.0004\n",
      "Epoch 5, Loss: 0.0002\n",
      "Epoch 6, Loss: 0.0002\n",
      "Epoch 7, Loss: 7.4614\n"
     ]
    }
   ],
   "source": [
    "model = SymbolicMusicTransformer()\n",
    "train_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ee4ff",
   "metadata": {},
   "source": [
    "## Test model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72d1c3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected chord condition: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Randomly pick a chord condition between 0 and NUM_CHORDS-1\n",
    "random_chord_condition = torch.randint(0, NUM_CHORDS, (1,)).item()\n",
    "print(\"Randomly selected chord condition:\", random_chord_condition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3471b",
   "metadata": {},
   "source": [
    "Built initial input to feed to the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9318067",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64  # (or whatever your model uses)\n",
    "\n",
    "pitch_input = torch.zeros((1, SEQ_LEN), dtype=torch.long)\n",
    "velocity_input = torch.zeros((1, SEQ_LEN), dtype=torch.long)\n",
    "duration_input = torch.zeros((1, SEQ_LEN), dtype=torch.long)\n",
    "instrument_input = torch.zeros((1, SEQ_LEN), dtype=torch.long)\n",
    "chord_input = torch.full((1, SEQ_LEN), random_chord_condition, dtype=torch.long)  # filled with the chosen chord\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966d970",
   "metadata": {},
   "source": [
    "Run the model to generate outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9162d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pitch_logits, velocity_logits, duration_logits, instrument_logits = model(\n",
    "        pitch_input, velocity_input, duration_input, chord_input, instrument_input\n",
    "    )\n",
    "\n",
    "# Take the argmax (highest probability) for each time step\n",
    "generated_pitch = pitch_logits.argmax(-1).squeeze(0).cpu().numpy()\n",
    "generated_velocity = velocity_logits.argmax(-1).squeeze(0).cpu().numpy()\n",
    "generated_duration = duration_logits.argmax(-1).squeeze(0).cpu().numpy()\n",
    "generated_instrument = instrument_logits.argmax(-1).squeeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349e7cf",
   "metadata": {},
   "source": [
    "Convert the generated sequence into MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8165256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file saved to: /Users/yang/Documents/generated_sample.mid\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "\n",
    "def save_generated_midi(pitches, velocities, durations, instruments, output_path=\"./generated_sample.mid\"):\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    instrument_map = {}\n",
    "\n",
    "    time = 0.0\n",
    "    for p, v, d, i in zip(pitches, velocities, durations, instruments):\n",
    "        start_time = time\n",
    "        dur_sec = (d + 1) * 0.1  # Each discrete duration bucket is 0.1 second\n",
    "        end_time = start_time + dur_sec\n",
    "\n",
    "        velocity = int(min(max(v * 4 + 30, 1), 127))  # Map velocity bucket to actual value\n",
    "\n",
    "        if i not in instrument_map:\n",
    "            instrument_map[i] = pretty_midi.Instrument(program=int(i))\n",
    "\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(p),\n",
    "            start=start_time,\n",
    "            end=end_time\n",
    "        )\n",
    "        instrument_map[i].notes.append(note)\n",
    "\n",
    "        time += dur_sec  # move forward in time\n",
    "\n",
    "    # Add instruments into MIDI\n",
    "    for inst in instrument_map.values():\n",
    "        midi.instruments.append(inst)\n",
    "\n",
    "    midi.write(output_path)\n",
    "    print(f\"MIDI file saved to: {output_path}\")\n",
    "\n",
    "# Save generated music\n",
    "#output_path = \"./generated_test_sample_v1.mid\"\n",
    "output_path = \"/Users/yang/Documents/generated_sample.mid\"\n",
    "save_generated_midi(generated_pitch, generated_velocity, generated_duration, generated_instrument, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4134d",
   "metadata": {},
   "source": [
    "Multiple-chord generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fd60818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate symbolic music conditioned on multiple chords\n",
    "import torch\n",
    "import pretty_midi\n",
    "\n",
    "def generate_music_with_chord_sequence_mul(model, chord_list, output_path=\"generated_multi_chord_sample.mid\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        T = SEQ_LEN\n",
    "\n",
    "        # Build the chord sequence (repeat each chord equally)\n",
    "        frames_per_chord = T // len(chord_list)\n",
    "        chord_sequence = []\n",
    "        for chord in chord_list:\n",
    "            chord_sequence += [chord] * frames_per_chord\n",
    "\n",
    "        # If not perfectly divisible, pad with the last chord\n",
    "        if len(chord_sequence) < T:\n",
    "            chord_sequence += [chord_list[-1]] * (T - len(chord_sequence))\n",
    "\n",
    "        chord_input = torch.tensor(chord_sequence, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Initialize zero inputs for pitch, velocity, duration, instrument\n",
    "        pitch_input = torch.zeros((1, T), dtype=torch.long)\n",
    "        velocity_input = torch.zeros((1, T), dtype=torch.long)\n",
    "        duration_input = torch.zeros((1, T), dtype=torch.long)\n",
    "        instrument_input = torch.zeros((1, T), dtype=torch.long)\n",
    "\n",
    "        # Model forward\n",
    "        pitch_logits, velocity_logits, duration_logits, instrument_logits = model(\n",
    "            pitch_input, velocity_input, duration_input, chord_input, instrument_input\n",
    "        )\n",
    "\n",
    "        # Take argmax to get predicted notes\n",
    "        generated_pitch = pitch_logits.argmax(-1).squeeze(0).tolist()\n",
    "        generated_velocity = velocity_logits.argmax(-1).squeeze(0).tolist()\n",
    "        generated_duration = duration_logits.argmax(-1).squeeze(0).tolist()\n",
    "        generated_instrument = instrument_logits.argmax(-1).squeeze(0).tolist()\n",
    "\n",
    "        # Save to MIDI\n",
    "        save_generated_midi(\n",
    "            generated_pitch,\n",
    "            generated_velocity,\n",
    "            generated_duration,\n",
    "            generated_instrument,\n",
    "            output_path=output_path\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fade26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_midi(pitch_seq, velocity_seq, duration_seq, instrument_seq, output_path=\"generated_output.mid\"):\n",
    "    midi = pretty_midi.PrettyMIDI()\n",
    "    instrument_map = {}\n",
    "\n",
    "    time = 0.0\n",
    "    for p, v, d, i in zip(pitch_seq, velocity_seq, duration_seq, instrument_seq):\n",
    "        dur_sec = (d + 1) * 0.1  # duration bucket to seconds\n",
    "        velocity_clipped = min(max(int(v * 4 + 30), 1), 127)\n",
    "\n",
    "        start_time = time\n",
    "        end_time = time + dur_sec\n",
    "\n",
    "        if i not in instrument_map:\n",
    "            program_num = int(i) if i < 127 else 127\n",
    "            instrument_map[i] = pretty_midi.Instrument(program=program_num)\n",
    "\n",
    "        note = pretty_midi.Note(velocity=velocity_clipped, pitch=p, start=start_time, end=end_time)\n",
    "        instrument_map[i].notes.append(note)\n",
    "        time += dur_sec\n",
    "\n",
    "    for inst in instrument_map.values():\n",
    "        midi.instruments.append(inst)\n",
    "\n",
    "    midi.write(output_path)\n",
    "    print(f\"MIDI file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b87c022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI file saved to: /Users/yang/Documents/generated_sample3.mid\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Assuming model is already trained or initialized\n",
    "chord_list = [0, 5, 7, 2]  # C major, F major, G major, D minor\n",
    "output_path = \"/Users/yang/Documents/generated_sample3.mid\"\n",
    "generate_music_with_chord_sequence_mul(model, chord_list, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58868c5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86dc2352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a282d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you want to switch chord every 16 frames\n",
    "chords = [0, 5, 7, 9]  # C, F, G, A (example)\n",
    "\n",
    "# Repeat each chord for 16 frames\n",
    "chord_sequence = []\n",
    "for c in chords:\n",
    "    chord_sequence += [c] * 16\n",
    "\n",
    "# Ensure length matches SEQ_LEN\n",
    "chord_sequence = chord_sequence[:SEQ_LEN]  \n",
    "\n",
    "# Convert to tensor\n",
    "chord_input = torch.tensor(chord_sequence, dtype=torch.long).unsqueeze(0)  # shape (1, SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e080954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    }
   ],
   "source": [
    "print(set(generated_instrument))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e72fc4",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ba1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fbd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3047b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbolic Music Transformer (MIDI-like model)\n",
    "# Pitch, Velocity, Duration, Instrument embeddings + Chord conditioning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "D_MODEL = 256  # Embedding dimension\n",
    "NUM_PITCHES = 128\n",
    "NUM_VELOCITIES = 32\n",
    "NUM_DURATIONS = 64\n",
    "NUM_CHORDS = 50\n",
    "NUM_INSTRUMENTS = 128  # Standard MIDI program count\n",
    "SEQ_LEN = 64\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Class with Instrument Awareness\n",
    "# -----------------------------\n",
    "class SymbolicMusicDataset(Dataset):\n",
    "    def __init__(self, midi_folder, max_files=100):\n",
    "        self.samples = []\n",
    "        self._collect_samples(midi_folder, max_files)\n",
    "\n",
    "    def _collect_samples(self, midi_folder, max_files):\n",
    "        midi_paths = list(Path(midi_folder).rglob(\"*.mid\"))[:max_files]\n",
    "        for midi_path in midi_paths:\n",
    "            try:\n",
    "                midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "                for inst in midi.instruments:\n",
    "                    for note in inst.notes:\n",
    "                        pitch = note.pitch\n",
    "                        velocity = min(note.velocity // 4, NUM_VELOCITIES - 1)\n",
    "                        duration = min(int((note.end - note.start) / 0.1), NUM_DURATIONS - 1)\n",
    "                        chord = random.randint(0, NUM_CHORDS - 1)  # placeholder\n",
    "                        instrument = inst.program if not inst.is_drum else 0\n",
    "                        self.samples.append((pitch, velocity, duration, chord, instrument))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {midi_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples) // SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx * SEQ_LEN\n",
    "        s = self.samples[idx:idx+SEQ_LEN]\n",
    "        pitch = torch.tensor([x[0] for x in s])\n",
    "        velocity = torch.tensor([x[1] for x in s])\n",
    "        duration = torch.tensor([x[2] for x in s])\n",
    "        chord = torch.tensor([x[3] for x in s])\n",
    "        instrument = torch.tensor([x[4] for x in s])\n",
    "        return pitch, velocity, duration, chord, instrument\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definition\n",
    "# -----------------------------\n",
    "class SymbolicTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.pitch_embed = nn.Embedding(NUM_PITCHES, D_MODEL)\n",
    "        self.velocity_embed = nn.Embedding(NUM_VELOCITIES, D_MODEL)\n",
    "        self.duration_embed = nn.Embedding(NUM_DURATIONS, D_MODEL)\n",
    "        self.chord_embed = nn.Embedding(NUM_CHORDS, D_MODEL)\n",
    "        self.instrument_embed = nn.Embedding(NUM_INSTRUMENTS, D_MODEL)\n",
    "        self.pos_embed = nn.Embedding(SEQ_LEN, D_MODEL)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=D_MODEL, nhead=8, dim_feedforward=512)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "        self.pitch_out = nn.Linear(D_MODEL, NUM_PITCHES)\n",
    "        self.velocity_out = nn.Linear(D_MODEL, NUM_VELOCITIES)\n",
    "        self.duration_out = nn.Linear(D_MODEL, NUM_DURATIONS)\n",
    "        self.instrument_out = nn.Linear(D_MODEL, NUM_INSTRUMENTS)\n",
    "\n",
    "    def forward(self, pitch, velocity, duration, chord, instrument):\n",
    "        B, T = pitch.size()\n",
    "        pos = torch.arange(T, device=pitch.device).unsqueeze(0).expand(B, T)\n",
    "\n",
    "        x = self.pitch_embed(pitch) + \\\n",
    "            self.velocity_embed(velocity) + \\\n",
    "            self.duration_embed(duration) + \\\n",
    "            self.chord_embed(chord) + \\\n",
    "            self.instrument_embed(instrument) + \\\n",
    "            self.pos_embed(pos)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return self.pitch_out(x), self.velocity_out(x), self.duration_out(x), self.instrument_out(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "def train_model(data_dir):\n",
    "    dataset = SymbolicMusicDataset(midi_folder=data_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    model = SymbolicTransformer()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        for pitch, vel, dur, chord, instr in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            pitch_logits, vel_logits, dur_logits, instr_logits = model(pitch, vel, dur, chord, instr)\n",
    "\n",
    "            loss = F.cross_entropy(pitch_logits.view(-1, NUM_PITCHES), pitch.view(-1)) + \\\n",
    "                   F.cross_entropy(vel_logits.view(-1, NUM_VELOCITIES), vel.view(-1)) + \\\n",
    "                   F.cross_entropy(dur_logits.view(-1, NUM_DURATIONS), dur.view(-1)) + \\\n",
    "                   F.cross_entropy(instr_logits.view(-1, NUM_INSTRUMENTS), instr.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(\"/path/to/your/lakh/dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
